---
title: "Vice Data Analytics"
date: "`r Sys.Date()`"
author: "Endri Raco"
output:
  rmdformats::downcute:
    toc_depth: 3
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## specify the packages needed
if(!require(tidyverse)) install.packages('tidyverse', 
                                         repos = 'http://cran.us.r-project.org')
if(!require(dlookr)) install.packages('dlookr', 
                                      repos = 'http://cran.us.r-project.org')
if(!require(here)) install.packages('here', 
                                    repos = 'http://cran.us.r-project.org')
if(!require(hrbrthemes)) install.packages('hrbrthemes', 
                                          repos = 'http://cran.us.r-project.org')
if(!require(viridis)) install.packages('viridis', 
                                       repos = 'http://cran.us.r-project.org')
if(!require(ggridges)) install.packages('ggridges', 
                                        repos = 'http://cran.us.r-project.org')
if(!require(scales)) install.packages('scales', 
                                      repos = 'http://cran.us.r-project.org')
if(!require(dygraphs)) install.packages('dygraphs', 
                                        repos = 'http://cran.us.r-project.org')
if(!require(xts)) install.packages('xts', 
                                   repos = 'http://cran.us.r-project.org')
if(!require(ggExtra)) install.packages('ggExtra', 
                                       repos = 'http://cran.us.r-project.org')
if(!require(TSstudio)) install.packages('TSstudio', 
                                       repos = 'http://cran.us.r-project.org')
if(!require(dygraphs)) install.packages('dygraphs', 
                                       repos = 'http://cran.us.r-project.org')
if(!require(GGally)) install.packages('GGally', 
                                       repos = 'http://cran.us.r-project.org')
if(!require(vroom)) install.packages("vroom", 
                                        repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", 
                                      repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", 
                                      repos = "http://cran.us.r-project.org")
if(!require(tidytext)) install.packages("tidytext", 
                                      repos = "http://cran.us.r-project.org")
if(!require(textclean)) install.packages("textclean", 
                                      repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", 
                                      repos = "http://cran.us.r-project.org")
if(!require(wordcloud2)) install.packages("wordcloud2", 
                                    repos = "http://cran.us.r-project.org")
if(!require(tidytext)) install.packages("tidytext", 
                                    repos = "http://cran.us.r-project.org")

if(!require(textclean)) install.packages("textclean", 
                                    repos = "http://cran.us.r-project.org")
if(!require(tidystopwords)) install.packages("tidystopwords", 
                                    repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", 
                                    repos = "http://cran.us.r-project.org")
if(!require(quanteda)) install.packages("quanteda", 
                                    repos = "http://cran.us.r-project.org")
if(!require(topicmodels)) install.packages("topicmodels", 
                                    repos = "http://cran.us.r-project.org")
if(!require(ldatuning)) install.packages("ldatuning", 
                                    repos = "http://cran.us.r-project.org")
if(!require(lda)) install.packages("lda", 
                                    repos = "http://cran.us.r-project.org")

knitr::opts_chunk$set(message = FALSE, error = FALSE, warning = FALSE)
customColor = "#FFA07A"
```

## Introduction

The aim of this report is to answer following questions using data techniques:

1.  How company content strategy has shifted over time.

2.  Are all kinds of engagement beneficial for video popularity? Naturally, a more popular video will have more reactions of all kinds, but does a higher fraction of, say, "Angry" reactions, have a negative effect on video performance?

3.  Are there any topics, word combinations which always perform higher than average, or have been successful as of recently?

We will use dataset **vice_data_for_test_task** This dataset contains Facebook video data from the past three years. The data concerns posts from four pages belonging to VICE.

For all project calculations is used the following PC:

```{r pc}
print('Operating System:')
version
```


## Data preparation
### Importing data

```{r data-download}
data_path <- here("data", "vice_data_for_test_task.csv")
vice_data <- read_csv(data_path)
```

### A first glimpse

First, we make a check if our data format is indeed **data frame**:



```{r data-format, eval=TRUE}
# Check format
class(vice_data)
```

We see that **vice_data** data frame has `r nrow(vice_data)` rows and `r ncol(vice_data)` variables.



Now let's check the structure of **vice_data** data frame

```{r data-str, eval=TRUE}
# Check structure
glimpse(vice_data)
```

It is a good idea to check for dublicates in rows so to create a general idea about real amount of data.

 

```{r distinct_data, eval=TRUE,cache=TRUE}
# Distinct users, movies, genres
nrow(vice_data %>% distinct())
```

 

Let's repair the names of variables:

```{r nr_data, eval=TRUE,cache=TRUE}
# Name repair
vice_data_cl <- janitor::clean_names(vice_data)
```

Now time for checking problems in dataset previous turning to data analysis

```{r diag_data, eval=TRUE,cache=TRUE}
diagnose(vice_data_cl)
```

### Data Wrangling

When we diagnosed **vice_data_cl** data frame we noticed that **final_link**, **image_text**, **description**, **sponsor_id**, **sponsor_name**, **sponsor_category** variables have more than $90\%$ missing data. Also we can notice that **page_admin_top_country** variables has a single value **US** so it will not be included in analytics. Let's remove these variables

```{r rem_data, eval=TRUE,cache=TRUE}
vice_data_cl <- vice_data_cl %>% select(-c('final_link', 'image_text', 'description', 'sponsor_id', 'sponsor_name', 'sponsor_category'))
```

Next step is to turn our two variables **page_created** and **post_created** to the right date-time format. We will use Vilnius timezone where company is located.

```{r date-time, eval=TRUE,cache=TRUE}
vice_data_cl$page_created <- as.POSIXct(vice_data_cl$page_created, tz = 'Europe/Vilnius')
vice_data_cl$post_created <- as.POSIXct(vice_data_cl$post_created, tz = 'Europe/Vilnius')
vice_data_cl$video_length <- lubridate::period_to_seconds(lubridate::hms(vice_data_cl$video_length))
```

## Information Extracting
### Post Creation

#### Posting by year

```{r year_post, eval=TRUE,cache=TRUE}
vice_data_cl %>%  
  mutate(year = lubridate::year(post_created)) %>% 
  group_by(year) %>% summarise(freq = n()) -> year_freqs 
ggplot(year_freqs, aes(x=year, y=freq)) +
  geom_bar(fill = 'green', stat='identity') 
```

#### Posting by month

```{r month_post, eval=TRUE,cache=TRUE}
vice_data_cl %>%  
  mutate(year = lubridate::year(post_created)) %>% 
  mutate(month = lubridate::month(post_created, label=TRUE)) %>%   
  group_by(year, month) %>% 
  summarise(freq = n())  -> month_freqs 
# subset 2 months around flood
month_freqs %>%
  ggplot(aes(x = month, y = freq)) +
  geom_bar(stat = "identity", fill = "darkorchid4") +
  facet_wrap(~ year, ncol = 1) +
  labs(title = "Monthly Video Postings")
```

#### Posting by day

```{r day_post, eval=TRUE,cache=TRUE}
vice_data_cl %>%  
  mutate(year = lubridate::year(post_created)) %>% 
  mutate(day = lubridate::date(post_created)) %>% 
  group_by(year, day) %>% 
  summarise(freq = n())  -> day_freqs
ggplot(day_freqs, aes(x = day, y = freq)) + 
  geom_line(aes(color = factor(year))) 

```

#### Frequency of Daily Posting

```{r dayf_post, eval=TRUE,cache=TRUE}
source("https://raw.githubusercontent.com/iascchen/VisHealth/master/R/calendarHeat.R")
vcl <- vice_data_cl %>%  
  select(post_created) %>%  
  group_by(post_created) %>% 
  summarise(freq = n()) 

r2g <- c("#D61818", "#FFAE63", "#FFFFBD", "#B5E384")
calendarHeat(vcl$post_created, vcl$freq, ncolors = 99, color = "r2g", varname="Frequency of Daily Posting")
```
#### Monthly Average of Daily POsts

```{r}
vlc <- vice_data_cl %>%  
select(post_created) %>% 
count(post_created)  %>%
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))


by_month <- vlc %>%
  group_by(Start.Month) %>%           
  summarise(av_posts = mean(n)) 

ggplot( data = by_month, 
aes(x = Start.Month, y = av_posts, fill=as.factor(lubridate::year(Start.Month)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Monthly Average of Daily Posts", x=NULL,  y="Number of Posts") + 
  theme_minimal() +
  theme(legend.position = "none") 
```


#### Weekly Average of Daily POsts


```{r}
vlc <- vice_data_cl %>%  
select(post_created) %>% 
count(post_created)  %>%
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))


by_week <- vlc %>%
  group_by(Start.Week) %>%           
  summarise(av_posts = mean(n)) 

ggplot( data = by_week, 
aes(x = Start.Week, y = av_posts, fill=as.factor(lubridate::year(Start.Week)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Weekly Average of Daily Posts", x=NULL,  y="Number of Posts") + 
  theme_minimal() +
  theme(legend.position = "none") 
```




#### Page Posting Over Time


```{r channel_post, eval=TRUE,cache=TRUE}
vice_data_cl %>%  select(post_created, page_name) %>% 
  group_by(post_created, page_name) %>% 
  summarise(freq = n()) %>%
  spread(key=page_name, value=freq) %>%
  select(-post_created) %>%
  ts_plot( title = "Page Posting over Time",
          Xtitle = "Time",
          Ytitle = "Number of Posts")
```


#### Monthly Average of Page Posting Over Time


```{r}
vlc<- vice_data_cl %>%  
select(post_created, page_name) %>% 
group_by(post_created, page_name) %>%  
count(post_created)  %>%
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))


vlc %>%
  group_by(Start.Month, page_name) %>%           
  summarise(av_posts = mean(n)) %>%
  spread(key=page_name, value=av_posts) %>%
  ts_plot( title = "Page Posting over Time",
          Xtitle = "Time",
          Ytitle = "Number of Posts")
```


#### Weekly Average of Page Posting Over Time


```{r}
vlc<- vice_data_cl %>%  
select(post_created, page_name) %>% 
group_by(post_created, page_name) %>%  
count(post_created)  %>%
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))


vlc %>%
  group_by(Start.Week, page_name) %>%           
  summarise(av_posts = mean(n)) %>%
  spread(key=page_name, value=av_posts) %>%
  ts_plot( title = "Page Posting over Time",
          Xtitle = "Time",
          Ytitle = "Number of Posts")
```

### Post Views
#### Daily Post Views Over Time

View count is the total number of people who have viewed your video.

Facebook measure a view by checking if someone views your video for 3 seconds (same for Live videos)

View count can be considered more of a vanity metric, as the number of views don't really affect your bottom line if no other action is taken. However, this still shows us that we need to make those first 3-30 seconds hyper-engaging in order to reel a viewer in.

```{r view_count, eval=TRUE,cache=TRUE}
don <- xts(x = vice_data_cl$post_views, order.by = vice_data_cl$post_created)
# Finally the plot
p <- dygraph(don, main = "Post Views Over Time", 
        ylab = "Number of Views") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
p 
```

#### Daily Total Views Over Time


```{r view_total, eval=TRUE,cache=TRUE}
don <- xts(x = vice_data_cl$total_views, order.by = vice_data_cl$post_created)
# Finally the plot
p <- dygraph(don, main = "Total Views Over Time", 
        ylab = "Number of Views") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
p 
```



#### Monthly Average of Views Over Time

```{r}
vlc <- vice_data_cl %>%  
select(post_created, post_views) %>%
group_by(post_created) %>%  
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))


by_month <- vlc %>%
  group_by(Start.Month) %>%           
  summarise(av_views = mean(post_views)) 

ggplot( data = by_month, 
aes(x = Start.Month, y = av_views, fill=as.factor(lubridate::year(Start.Month)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Monthly Average of Views Over Time", x=NULL,  y="Number of Views") + 
  theme_minimal() +
  theme(legend.position = "none") 
```








#### Weekly Average of Views Over Time

```{r}
vlc <- vice_data_cl %>%  
select(post_created, post_views) %>%
group_by(post_created) %>%  
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))


by_week <- vlc %>%
  group_by(Start.Week) %>%           
  summarise(av_views = mean(post_views)) 

ggplot( data = by_week, 
aes(x = Start.Week, y = av_views, fill=as.factor(lubridate::year(Start.Week)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Weekly Average of Views Over Time", x=NULL,  y="Number of Views") + 
  theme_minimal() +
  theme(legend.position = "none") 
```






#### Total Views for all Crossposts Over Time


```{r view_cross, eval=TRUE,cache=TRUE}
don <- xts(x = vice_data_cl$total_views_for_all_crossposts, order.by = vice_data_cl$post_created)
# Finally the plot
p <- dygraph(don, main = "Total Views for all Crossposts Over Time", 
        ylab = "Number of Views") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
p 
```



#### Monthly Average of Total Views for all Crossposts Over Time


```{r}
vlc <- vice_data_cl %>%  
select(post_created, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))


by_month <- vlc %>%
  group_by(Start.Month) %>%           
  summarise(av_views = mean(total_views_for_all_crossposts)) 

ggplot( data = by_month, 
aes(x = Start.Month, y = av_views, fill=as.factor(lubridate::year(Start.Month)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Monthly Average of Total Views for all Crossposts Over Time", x=NULL,  y="Number of Views") + 
  theme_minimal() +
  theme(legend.position = "none") 
```








#### Weekly Average of Total Views for all Crossposts Over Time

```{r}
vlc <- vice_data_cl %>%  
select(post_created, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))


by_week <- vlc %>%
  group_by(Start.Week) %>%           
  summarise(av_views = mean(total_views_for_all_crossposts)) 

ggplot( data = by_week, 
aes(x = Start.Week, y = av_views, fill=as.factor(lubridate::year(Start.Week)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Weekly Average of Total Views for all Crossposts Over Time", x=NULL,  y="Number of Views") + 
  theme_minimal() +
  theme(legend.position = "none") 
```



#### Total Views vs Total Views for All Crossposts Overtime

```{r view_comp, eval=TRUE,cache=TRUE}
vice_data_cl %>%  select(post_created, total_views, total_views_for_all_crossposts) %>% 
  group_by(post_created) %>% 
  ts_plot(title = " Total views vs Total Views for All crossposts Over Time",
          Xtitle = "Time",
          Ytitle = "Frequency")
```


#### Monthly Average of Total Views vs Total Views for All Crossposts Overtime

```{r}
vlc <- vice_data_cl %>%  
select(post_created, total_views, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))

vlc %>%
  group_by(Start.Month) %>%           
  summarise(av_tot_views = mean(total_views), av_tot_cviews = mean(total_views_for_all_crossposts)) %>%
  ts_plot(title = " Monthly Average of Total Views vs Total Views for All Crossposts Overtime",
          Xtitle = "Time",
          Ytitle = "Frequency")
```



#### Weekly Average of Total Views vs Total Views for All Crossposts Overtime

```{r}
vlc <- vice_data_cl %>%  
select(post_created, total_views, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))

vlc %>%
  group_by(Start.Week) %>%           
  summarise(av_tot_views = mean(total_views), av_tot_cviews = mean(total_views_for_all_crossposts)) %>%
  ts_plot(title = " Weekly Average of Total Views vs Total Views for All Crossposts Overtime",
          Xtitle = "Time",
          Ytitle = "Frequency")
```


### Video Length

#### Posted Video Length

```{r video_len, eval=TRUE,cache=TRUE}
vice_data_cl %>%
  filter( video_length < 1200 ) %>%
  ggplot( aes(x= video_length)) +
  geom_histogram( binwidth=10, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Histogram of Posted Video Length ") +
  theme_ipsum() +
  theme(
    plot.title = element_text(size=15)
  ) + 
  scale_y_continuous(breaks=seq(0,1000,50)) + 
  scale_x_continuous(breaks=seq(0,1200,100))
```

#### Length of Video Posts in Time

```{r vl_time, eval=TRUE,cache=TRUE}
vice_data_cl %>%  select(post_created, video_length) %>%
  filter( video_length < 1200 ) %>%
  mutate(year = lubridate::year(post_created)) %>%
select(year, video_length) %>%
ggplot(aes(x=video_length, fill = as.factor(year)))+
  geom_histogram( color='#e9ecef', alpha=0.6) + 
labs(title = "Posted Video Lengths in Years") +
xlab('Video Length') +
ylab('Frequency of Video Posts') +
guides(fill=guide_legend(title="Years"))  
```



#### Monthly Average of Video Length

```{r}
vlc <- vice_data_cl %>%  
select(post_created, video_length) %>% 
group_by(post_created)  %>%
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"))


by_month <- vlc %>%
  group_by(Start.Month) %>%           
  summarise(av_length = mean(video_length)) 

ggplot( data = by_month, 
aes(x = Start.Month, y = av_length, fill=as.factor(lubridate::year(Start.Month)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Monthly Average of Video Length", x=NULL,  y="Video Length") + 
  theme_minimal() +
  theme(legend.position = "none") +
scale_y_continuous(breaks=seq(0, 1000,100), limits=c(0,1000))
```


#### Weekly Average of Video Length

```{r}
vlc <- vice_data_cl %>%  
select(post_created, video_length) %>% 
group_by(post_created)  %>%
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"))


by_week <- vlc %>%
  group_by(Start.Week) %>%           
  summarise(av_length = mean(video_length)) 

ggplot( data = by_week, 
aes(x = Start.Week, y = av_length, fill=as.factor(lubridate::year(Start.Week)))) +
geom_col() +  
scale_fill_brewer(palette = "Paired") +
labs(title="Weekly Average of Video Length", x=NULL,  y="Video Length") + 
  theme_minimal() +
  theme(legend.position = "none") +
scale_y_continuous(breaks=seq(0, 1000,100), limits=c(0,1000))
```




### Engagement

Video engagement includes the comments and likes that video content generates.

It's a good idea to see how many people are actually taking action on your video, but more than that, company pay attention to the types of comments is getting.

#### Daily User Activity

```{r act_day, eval=TRUE,cache=TRUE}
don <- xts(x = vice_data_cl$total_interactions, order.by = vice_data_cl$post_created)
# Finally the plot
p <- dygraph(don, main = "Total Interactions Over Time", 
        ylab = "Number of Views") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1, drawGrid = FALSE, colors="#D8AE5A") %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
p 
```

#### Daily User Activity

```{r act_year, eval=TRUE,cache=TRUE}
vice_data_cl %>%  select(post_created, total_interactions) %>%
filter( total_interactions < 5000 ) %>%
mutate(year = lubridate::year(post_created)) %>%
select(year, total_interactions) %>%
ggplot(aes(x=total_interactions, fill = as.factor(year)))+
geom_histogram( binwidth=200,color="#e9ecef", alpha=0.9) +
ggtitle("Histogram of Total Interactions During Years ") +
theme_ipsum() +
theme(
plot.title = element_text(size=15)
) +
xlab('Total Interactions') +
ylab('Frequency of Total Interactions') +
guides(fill=guide_legend(title="Years"))   
```

#### Relationship between different user reactions

```{r inter_rel, eval=TRUE,cache=TRUE}
vice_data_cl %>% 
select(likes, comments, shares, love, wow, haha, sad, angry, care) %>%
ggpairs()  
```




#### Comparision of weekly user interaction rates


```{r week_inter, eval=TRUE,cache=TRUE}
vlc <- vice_data_cl %>%  
select(post_created, likes, comments, shares, love, wow, haha, sad, angry, care, total_interactions) %>%
group_by(post_created) %>%  
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"),
       like_ratio = likes/total_interactions,
       comments_ratio = comments/total_interactions,
       shares_ratio = shares/total_interactions,
       love_ratio = love/total_interactions,
       wow_ratio = wow/total_interactions,
       haha_ratio = haha/total_interactions,
       sad_ratio = sad/total_interactions,
       angry_ratio = angry/total_interactions,
       care_ratio = care/total_interactions) %>%
select(post_created, like_ratio, comments_ratio, shares_ratio, love_ratio, wow_ratio, haha_ratio, sad_ratio, angry_ratio, care_ratio, Start.Week)  

vlc %>%
  group_by(Start.Week) %>%           
  summarise(
    av_like_ratio = mean(like_ratio),
    av_comments_ratio = mean(comments_ratio),
    av_shares_ratio = mean(shares_ratio),
    av_love_ratio = mean(love_ratio),
    av_wow_ratio = mean(wow_ratio),
    av_haha_ratio = mean(haha_ratio),
    av_sad_ratio = mean(sad_ratio),
    av_angry_ratio = mean(angry_ratio),
    av_care_ratio  = mean(care_ratio)) %>%
  ts_plot(title = " Comparision of weekly user interaction rates Over Time",
          Xtitle = "Time",
          Ytitle = "")
```


#### Comparision of monthly user interaction rates


```{r month_inter, eval=TRUE,cache=TRUE}
vlc <- vice_data_cl %>%  
select(post_created, likes, comments, shares, love, wow, haha, sad, angry, care, total_interactions) %>%
group_by(post_created) %>%  
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"),
       like_ratio = likes/total_interactions,
       comments_ratio = comments/total_interactions,
       shares_ratio = shares/total_interactions,
       love_ratio = love/total_interactions,
       wow_ratio = wow/total_interactions,
       haha_ratio = haha/total_interactions,
       sad_ratio = sad/total_interactions,
       angry_ratio = angry/total_interactions,
       care_ratio = care/total_interactions) %>%
select(post_created, like_ratio, comments_ratio, shares_ratio, love_ratio, wow_ratio, haha_ratio, sad_ratio, angry_ratio, care_ratio, Start.Month)  

vlc %>%
  group_by(Start.Month) %>%           
  summarise(
    av_like_ratio = mean(like_ratio),
    av_comments_ratio = mean(comments_ratio),
    av_shares_ratio = mean(shares_ratio),
    av_love_ratio = mean(love_ratio),
    av_wow_ratio = mean(wow_ratio),
    av_haha_ratio = mean(haha_ratio),
    av_sad_ratio = mean(sad_ratio),
    av_angry_ratio = mean(angry_ratio),
    av_care_ratio  = mean(care_ratio)) %>%
  ts_plot(title = " Comparision of monthly user interaction rates Over Time",
          Xtitle = "Time",
          Ytitle = "")
```

#### Monthy effect of Angry Reaction in Video Performance


```{r month_inter_angry, eval=TRUE,cache=TRUE}
library(ggpubr)
vlc <- vice_data_cl %>%  
select(post_created, angry, total_interactions, likes_at_posting, followers_at_posting, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month"),
       angry_ratio = angry/total_interactions) %>%
select(post_created, angry_ratio, Start.Month,  total_interactions, likes_at_posting, followers_at_posting, total_views_for_all_crossposts)  

vlc <- vlc %>%
  group_by(Start.Month) %>%           
  summarise(
    av_angry_ratio = mean(angry_ratio),
    av_total_interactions  = mean(total_interactions),
    av_likes_at_posting  = mean(likes_at_posting),
    av_total_views_for_all_crossposts  = mean(total_views_for_all_crossposts) ) 


vlc1 <- vlc %>% select(Start.Month, av_angry_ratio)
vlc2 <- vlc %>% select(Start.Month, av_total_interactions)
vlc3 <- vlc %>% select(Start.Month, av_likes_at_posting)
vlc4 <- vlc %>% select(Start.Month, av_total_views_for_all_crossposts)

p1 <- ggplot(vlc1, aes(x=Start.Month, av_angry_ratio)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Angry Ratio by Month") +
  xlab("Time")


p2 <- ggplot(vlc2, aes(x=Start.Month, av_total_interactions)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Monthly Average of Total Interactions") +
  xlab("Time")

p3 <- ggplot(vlc3, aes(x=Start.Month, av_likes_at_posting)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Monthly Average of Likes at Posting") +
  xlab("Time")

p4 <- ggplot(vlc4, aes(x=Start.Month, av_total_views_for_all_crossposts)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Monthly Average of Total Crossposts") +
  xlab("Time")

ggarrange(p1,p2,p3,p4)


```

#### Weekly of Angry Reaction in Video Performance


```{r week_inter_angry, eval=TRUE,cache=TRUE}
library(ggpubr)
vlc <- vice_data_cl %>%  
select(post_created, angry, total_interactions, likes_at_posting, followers_at_posting, total_views_for_all_crossposts) %>%
group_by(post_created) %>%  
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week"),
       angry_ratio = angry/total_interactions) %>%
select(post_created, angry_ratio, Start.Week,  total_interactions, likes_at_posting, followers_at_posting, total_views_for_all_crossposts)  

vlc <- vlc %>%
  group_by(Start.Week) %>%           
  summarise(
    av_angry_ratio = mean(angry_ratio),
    av_total_interactions  = mean(total_interactions),
    av_likes_at_posting  = mean(likes_at_posting),
    av_total_views_for_all_crossposts  = mean(total_views_for_all_crossposts) ) 


vlc1 <- vlc %>% select(Start.Week, av_angry_ratio)
vlc2 <- vlc %>% select(Start.Week, av_total_interactions)
vlc3 <- vlc %>% select(Start.Week, av_likes_at_posting)
vlc4 <- vlc %>% select(Start.Week, av_total_views_for_all_crossposts)

p1 <- ggplot(vlc1, aes(x=Start.Week, av_angry_ratio)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Angry Ratio by Week") +
  xlab("Time")


p2 <- ggplot(vlc2, aes(x=Start.Week, av_total_interactions)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Weekly Average of Total Interactions") +
  xlab("Time")

p3 <- ggplot(vlc3, aes(x=Start.Week, av_likes_at_posting)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Weekly Average of Likes at Posting") +
  xlab("Time")

p4 <- ggplot(vlc4, aes(x=Start.Week, av_total_views_for_all_crossposts)) +
  geom_line( color="steelblue") + 
  geom_point() +
  xlab("") +
  theme_ipsum() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  ylab("Weekly Average of Total Crossposts") +
  xlab("Time")

ggarrange(p1,p2,p3,p4)


```


### Social shares

One of main goals for video content should be social shares. This widens audience exponentially, increasing brand awareness and potentially bringing in new leads.

#### Video Share Status -- owned vs crosspost

```{r, video_share, eval=TRUE,cache=TRUE}
vlc<- vice_data_cl %>% 
  select(post_created, video_share_status) %>% 
  group_by(post_created, video_share_status) %>% 
  summarise(freq = n()) %>%
  spread(key=video_share_status, value=freq) %>%
  select(crosspost, owned, share)
ts_plot(vlc,         title = "Video Share Status Over Time",
        Xtitle = "Time",
        Ytitle = "Frequency")
```


#### Monthly Average Comparision of Video Share Status -- owned vs crosspost

```{r, month_video_comp, eval=TRUE,cache=TRUE}
vlc<- vice_data_cl %>% 
  select(post_created, video_share_status) %>% 
  group_by(post_created, video_share_status) %>% 
  summarise(freq = n()) %>%
  spread(key=video_share_status, value=freq) %>%
  select(crosspost, owned, share) %>%
mutate(Start.Month = lubridate::floor_date(post_created, unit = "month")) %>%
  select(Start.Month, crosspost, owned, share)

vlc %>%
  group_by(Start.Month) %>%           
  summarise(
    crosspost_ratio = mean(crosspost),
    owned_ratio = mean(owned),
    share_ratio = mean(share)) %>%
  ts_plot(title = " Monthly Comparision of Video Share Status",
          Xtitle = "Time",
          Ytitle = "")
```

#### Weekly Average Comparision of Video Share Status -- owned vs crosspost

```{r, week_video_comp, eval=TRUE,cache=TRUE}
vlc<- vice_data_cl %>% 
  select(post_created, video_share_status) %>% 
  group_by(post_created, video_share_status) %>% 
  summarise(freq = n()) %>%
  spread(key=video_share_status, value=freq) %>%
  select(crosspost, owned, share) %>%
mutate(Start.Week = lubridate::floor_date(post_created, unit = "week")) %>%
  select(Start.Week, crosspost, owned, share)

vlc %>%
  group_by(Start.Week) %>%           
  summarise(
    crosspost_ratio = mean(crosspost),
    owned_ratio = mean(owned),
    share_ratio = mean(share)) %>%
  ts_plot(title = " Monthly Comparision of Video Share Status",
          Xtitle = "Time",
          Ytitle = "")
```




## Natural Language Processing
### Data Cleaning

Let's start with data cleaning for variables **message** and **link_text**. Because of text specifics, cleaning is done in two stages:

For **message** :


```{r, cleaning-text, eval=TRUE,cache=TRUE}
text <- as.character(vice_data_cl$message) %>%
tolower() %>%
  # remove non-word characters
  str_replace_all("[^[:alpha:][:space:]]*", "")  %>%
  tm::removePunctuation() %>%
  stringr::str_squish() %>%
  stringr::str_split(" ") %>%
  textclean::replace_non_ascii(replacement = "") %>%
  unlist()

message_corpus <- Corpus(VectorSource(na.omit(text))) 

english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")


message_clean_corpus <- tm_map(message_corpus, content_transformer(tolower))
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(removeWords), english_stopwords)
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(removePunctuation), preserve_intra_word_dashes = TRUE)
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(removeNumbers))
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(stemDocument), language = "en")
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(stripWhitespace))
message_clean_corpus <- tm_map(message_clean_corpus, content_transformer(stemDocument))


clean_fun <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

message_clean_corpus <- tm_map(message_clean_corpus, clean_fun, "/")
message_clean_corpus <- tm_map(message_clean_corpus, clean_fun , "@")
message_clean_corpus <- tm_map(message_clean_corpus, clean_fun , "\\|")

```

Now the same for **link_text**


```{r, cleaning-ltext, eval=TRUE,cache=TRUE}
link_text <- as.character(vice_data_cl$link_text) %>%
tolower() %>%
  # remove non-word characters
  str_replace_all("[^[:alpha:][:space:]]*", "")  %>%
  tm::removePunctuation() %>%
  stringr::str_squish() %>%
  stringr::str_split(" ") %>%
  textclean::replace_non_ascii(replacement = "") %>%
  unlist()

ltext_corpus <- Corpus(VectorSource(na.omit(link_text))) 

english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")


ltext_clean_corpus <- tm_map(ltext_corpus, content_transformer(tolower))
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(removeWords), english_stopwords)
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(removePunctuation), preserve_intra_word_dashes = TRUE)
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(removeNumbers))
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(stemDocument), language = "en")
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(stripWhitespace))
ltext_clean_corpus <- tm_map(ltext_clean_corpus, content_transformer(stemDocument))


clean_fun <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

ltext_clean_corpus <- tm_map(ltext_clean_corpus, clean_fun, "/")
ltext_clean_corpus <- tm_map(ltext_clean_corpus, clean_fun , "@")
ltext_clean_corpus <- tm_map(ltext_clean_corpus, clean_fun , "\\|")

```

### Topic Model

Now we will calculate topic model for both created corpuses. First we vectorize:

```{r, dtm, eval=TRUE,cache=TRUE}
dtm_message <- DocumentTermMatrix(message_clean_corpus, control = list(wordLengths = c(2, Inf)))
dtm_ltext <- DocumentTermMatrix(ltext_clean_corpus, control = list(wordLengths = c(2, Inf)))
```


Next, we determine the optimal number of topics from **dtm_message**:

```{r, message_topic_number, eval=TRUE,cache=TRUE}
# Remove zero elements to perform LDA
raw.sum=apply(dtm_message,1,FUN=sum) 
dtm_message=dtm_message[raw.sum!=0,]
# For message_dtm
message_result <- FindTopicsNumber(
  dtm_message,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(message_result)
```



Basically we look for parameters that minimize Arun and CaoJuan, or maximize Griffiths and Deveaud. For our purpose we take $k = 7$


```{r, lda_message, eval=TRUE,cache=TRUE}
raw.sum=apply(dtm_message,1,FUN=sum) 
dtm_message=dtm_message[raw.sum!=0,]
K <- 7
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
message_topicModel <- LDA(dtm_message, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
```


Below 20 most likely terms within the term probabilities beta of the inferred topics

```{r, popular_word_message, eval=TRUE,cache=TRUE}
terms(message_topicModel, 20)
```

Let's repeat the same procedure for the **link_message**


```{r, topic_number_ltext, eval=FALSE,cache=TRUE}
# Remove zero elements to perform LDA
raw.sum=apply(dtm_ltext,1,FUN=sum) 
dtm_ltext=dtm_ltext[raw.sum!=0,]
# For message_dtm
ltext_result <- FindTopicsNumber(
  dtm_ltext,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
FindTopicsNumber_plot(ltext_result)
```



For our purpose we take $k = 7$


```{r, lda_ltext, eval=TRUE,cache=TRUE}
raw.sum=apply(dtm_ltext,1,FUN=sum) 
dtm_ltext=dtm_ltext[raw.sum!=0,]
K <- 7
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
ltext_topicModel <- LDA(dtm_ltext, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
```


Below 20 most likely terms within the term probabilities beta of the inferred topics

```{r, popular_word_ltext, eval=TRUE,cache=TRUE}
terms(ltext_topicModel, 20)
```


Lets give some name to topics:

```{r, topic_names, eval=TRUE,cache=TRUE}
message_topic_name <- terms(message_topicModel, 7)
message_topicNames <- apply(message_topic_name, 2, paste, collapse=" ")
ltext_topic_name <- terms(ltext_topicModel, 7)
ltext_topicNames <- apply(ltext_topic_name, 2, paste, collapse=" ")
```

### Information Extraction

Lets visualize topic models:

```{r, tidy_lda, eval=TRUE,cache=TRUE}
message_lda <- tidy(message_topicModel)
ltext_lda <- tidy(ltext_topicModel)
```


Top 20 terms for each topic and then look at this information visually:


```{r, message_lda_plot, eval=TRUE,cache=TRUE}
message_top20_terms <- message_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 20, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 20 terms in each message LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
message_top20_terms

```

For **link_text**


```{r, ltext_lda_plot, eval=TRUE,cache=TRUE}
ltext_top20_terms <- ltext_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 20, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 20 terms in each link_text LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
ltext_top20_terms

```
Now we will use wordclouds to visualize our selected topics:

```{r, message_wordcloud, eval=TRUE,cache=TRUE}
message_post <- posterior(message_topicModel)
# our topic of interest
int_topic_message <- 1
## 100 terms
top100terms <- sort(message_post$terms[int_topic_message,], decreasing=TRUE)[1:100]
words <- names(top100terms)
probabilities <- sort(message_post$terms[int_topic_message,], decreasing=TRUE)[1:100]
mycolors <- brewer.pal(7, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```


Same for **link_text**

```{r, ltextr_wordcloud, eval=TRUE,cache=TRUE}
ltext_post <- posterior(ltext_topicModel)
# our topic of interest
int_topic_message <- 1
## 100 terms
top100terms <- sort(ltext_post$terms[int_topic_message,], decreasing=TRUE)[1:100]
words <- names(top100terms)
probabilities <- sort(ltext_post$terms[int_topic_message,], decreasing=TRUE)[1:100]
mycolors <- brewer.pal(7, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

The message topic model includes two  posterior probability distributions: a distribution $\theta$ over K topics within each document and a distribution $\beta$ over V terms within each topic, where V represents the length of the vocabulary of the collection. Let's check V in **dtm_message**:


```{r, message_post_dist, eval=TRUE,cache=TRUE}
nTerms(dtm_message) ## Our V value
message_post_result <- posterior(message_topicModel)
message_beta <- message_post_result$terms ## Our beta distribution
dim(message_beta)
message_theta <- message_post_result$topics  ## Our theta distribution
dim(message_theta) 
```

For **link_text**

```{r, ltext_post_dist, eval=TRUE,cache=TRUE}
nTerms(dtm_ltext) ## Our V value
ltext_post_result <- posterior(ltext_topicModel)
ltext_beta <- ltext_post_result$terms ## Our beta distribution
dim(ltext_beta)
ltext_theta <- ltext_post_result$topics  ## Our theta distribution
dim(ltext_theta)   
```



Let's check on the distribution of topics

```{r, id_sample, eval=TRUE,cache=TRUE}
sample_id<- c(5, 50, 150, 250)
lapply(message_clean_corpus[sample_id], as.character)
message_topic_top_5 <- terms(message_topicModel, 5)
message_topic_name <- apply(message_topic_top_5, 2, paste, collapse=" ")
N <- length(sample_id)
# Proportion for each sample
sample_prop_message <- message_theta[sample_id,]
colnames(sample_prop_message) <- message_topic_name
message_plot <- reshape::melt(cbind(data.frame(sample_prop_message), document = factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = message_plot, aes(variable, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```




Now we repeat the above steps for **link_text**

```{r, ltext_sample, eval=TRUE,cache=TRUE}
# Put some name for each topic
ltext_topic_top_5 <- terms(ltext_topicModel, 5)
ltext_topic_name <- apply(ltext_topic_top_5, 2, paste, collapse=" ")
N <- length(sample_id)
# Proportion for each sample
ltext_prop_message <- ltext_theta[sample_id,]
colnames(ltext_prop_message) <- ltext_topic_name
ltext_plot <- reshape::melt(cbind(data.frame(ltext_prop_message), document = factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = message_plot, aes(variable, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```



### Topic ranking

Now we try to get a more meaningful order of top terms per topic by re-ranking them with a specific score

```{r, message_rank, eval=TRUE,cache=TRUE}
message_topic_name <- apply(lda::top.topic.words(message_beta, 5, by.score = T), 2, paste, collapse = " ")
```

####  Rank-1 Methods


```{r, message_sort, eval=TRUE,cache=TRUE}
topic_count <- rep(0, K)
names(topic_count) <- message_topic_name
for (i in 1:nDocs(dtm_message)) {
  doc_topic <- message_theta[i, ] 
  high_topic <- order(doc_topic, decreasing = TRUE)[1] 
  topic_count[high_topic] <- topic_count[high_topic] + 1
}
sort(topic_count, decreasing = TRUE)
```




```{r, message_sort_result, eval=TRUE,cache=TRUE}
topic_sort <- sort(topic_count, decreasing = TRUE)
paste(topic_sort, ":", names(topic_sort))
```

Now we repeat steps for **link_text**



```{r, ltext_rank, eval=TRUE,cache=TRUE}
ltext_topic_name <- apply(lda::top.topic.words(ltext_beta, 5, by.score = T), 2, paste, collapse = " ")
topic_count <- rep(0, K)
names(topic_count) <- ltext_topic_name
for (i in 1:nDocs(dtm_ltext)) {
  doc_topic <- ltext_theta[i, ] 
  high_topic <- order(doc_topic, decreasing = TRUE)[1] 
  topic_count[high_topic] <- topic_count[high_topic] + 1
}
sort(topic_count, decreasing = TRUE)
```

```{r, ltext_sort, eval=TRUE,cache=TRUE}
topic_sort <- sort(topic_count, decreasing = TRUE)
paste(topic_sort, ":", names(topic_sort))
```

## Conclusions
### Question 1  How company content strategy has shifted over time

In this study, several metrics are taken into account, such as the number of posts, video views, user engagement, and page growth.

  1. Number of posts

We see that during 2018, the monthly averages of company postings are very similar except for a slight increase in November and December of this year. We can see the same trend during 2019 as well. There is a drop in monthly averages of company postings during 2020 that started to be significant since March and remains similar for all 2020, probably because of pandemic situation. There is an increase in monthly averages of postings in February 2021, but in a lower rate compare to 2019.

When it comes to daily postings, 2018 is characterized by active postings on Monday through Friday, with a lower frequency during weekends. An increase in posting frequency during weekends is noticed in November - December 2018 reaching the level of other daily weeks, and this trend remains unchanged during 2019 with a peak of postings in the third week of November, likely due to elections in US or sales season. Again 2020 shows a decreased frequency of postings which remains in these levels during whole 2020, with a slight increase in 2021.

Vice News is the page which seems to be favorite when it comes to postings, with a continuous increase from 2018 to start of 2020 followed by a sharp decrease in the number of posts.

Vice seems to be more stable with a rather smooth increase from 2018 to start of 2020, followed by not such as sharp decrease as Vice News and constantly improving by getting to the levels of 2019.

Vice TV has experienced an increase in number of posts only in the last months of 2018, and immediately after that started to experience a gradually decrease in postings.



  2. Video Views
  
When it comes to video views, company seems to have performed quite well with politics of slowing down postings number, and focusing more in user engagement and content. 2018 has an increase in monthly average total views, with some slowing during September - October. 2019 seems to be a very good year when it comes to the total view with a slower rate of increase from April to August. The increase trend seems to remains constant for first half of 2020 with a noticeable decrease in second half which seems to be recovered in first months in 2021. Company shows to be very focused on crossposts and views the video has amassed over all times it was posted. Average monthly crossposts show a constant high-rate increase from the start to July 2019 keeping a more constant rate during 2020 and 2021. The characteristic for average monthly cross-posts seems to be turbulence (frequent ups and downs), probably connected to video content.


  3. Video Length
  
Company seems to keep posting videos of length (3.5-4 min) and while frequency of postings (number of videos during day) can variate, company seems to maintain a rather constant weekly average of video length posting, with rare turbulence, connected probably with live streaming. A rather small increase in video length is noticed from second half of 2020 after some disrupters continuing in 2021.


  4. User Interaction
  
Users appear to the be faithful to pages during 2019. The last months of 2018 are characterized by some mixed feelings from users, showing a decrease in the number of likes, an increase in the number of comments, and a constant number of shares. More strong feelings like **sad**, **love**, and **angry** remain in constant very low rates which show that the user is not strongly negative affected by the content. The company seems to have reacted successfully to user feedback, because 2019 seems to be more commented on, while **likes** seem to stay constant, while an increase rate in **haha** shows more engaging content. The end of 2019 and the start of 2020 seem to show that the company has a good understanding of the willingness for content, reflected in higher like rates, lower level of comments, and constant rates for other feelings.
  

### Question 2  Are all kinds of engagement beneficial for video popularity?  

  
 It does not seem that all kinds of engagement are beneficial for video popularity, but this should be studied in different times. For example, plots show that **Angry** feeling has affected the page popularity measured in terms of **Average Interactions** and **Average Crossposts**. However, this effect seems to get unimportant and even comes in benefit in 2019 and on when page already has a constant flow of user interactions. Some reactions like **care** or **love** seem to be in very low rates to get a feeling about their importance in page performance.
  


### Question 3   Are there any topics, word combinations which always perform higher than average, or have been successful as of recently?


As it may not seem strange **Covid19**, **Trump Impeachment**, and **Black Matters** are the hottest topics during the period of study with Covid to be the main topic. Beside that people seems to continue following their interests such as **Cooking** or **Entertainment Series**. The combinations which seem to perform higher are **world year coronavirus countries**, **news pm tonight trump** and **tonight demand episode stories**
